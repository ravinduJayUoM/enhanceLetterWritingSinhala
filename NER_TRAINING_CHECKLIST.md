# üéì NER Model Training Checklist for Sinhala Letter Slot Extraction

## üìä **Current Status**

**Training Data Available:**
- ‚úÖ 80 training samples (64 annotated, 16 unannotated)
- ‚úÖ 20 validation samples (all annotated)
- ‚úÖ Transformer format already prepared
- ‚úÖ Training script exists (`finetune_ner_model.py`)
- ‚úÖ Base model: XLM-RoBERTa (multilingual, supports Sinhala)

**Entity Coverage:**
- ‚úÖ subject: 39 samples
- ‚úÖ recipient: 30 samples  
- ‚úÖ sender: 19 samples
- ‚ö†Ô∏è incident_date: 19 samples
- ‚ö†Ô∏è purpose: 19 samples
- ‚ö†Ô∏è contact_details: 11 samples
- ‚ö†Ô∏è details: 10 samples
- ‚ùå letter_type: 0 samples (CRITICAL GAP!)
- ‚ùå qualifications: 0 samples
- ‚ùå requested_action: 0 samples
- ‚ùå requested_items: 0 samples
- ‚ùå timeline: 0 samples

---

## üö® **Critical Issues to Address BEFORE Training**

### 1. **Missing Entity Types (High Priority)**
**Problem:** 5 entity types have ZERO training examples:
- `letter_type` - Most important! (‡∂Ö‡∂∫‡∂Ø‡∑î‡∂∏‡∑ä‡∂¥‡∂≠‡∑ä, ‡∂â‡∂Ω‡∑ä‡∂Ω‡∑ì‡∂∏‡∑ä, ‡∂¥‡∑ê‡∂∏‡∑í‡∂´‡∑í‡∂Ω‡∑ä‡∂Ω, etc.)
- `qualifications` - For job applications
- `requested_action` - What you want recipient to do
- `requested_items` - For donation requests
- `timeline` - Deadlines, date ranges

**Solution:** Need to annotate at least 15-20 examples per entity type

### 2. **Insufficient Training Data**
**Problem:** 64 annotated samples is very small for deep learning
- Minimum recommended: 200-500 samples
- Optimal: 1000+ samples

**Current coverage is insufficient for production-quality NER**

### 3. **Unbalanced Dataset**
**Problem:** Entity distribution is heavily skewed
- subject: 39 samples ‚úì
- recipient: 30 samples ‚úì
- sender/purpose/incident_date: ~19 samples ‚ö†Ô∏è
- Other entities: 0-11 samples ‚ùå

**Impact:** Model will be biased toward common entities, fail on rare ones

---

## ‚úÖ **Step-by-Step Training Checklist**

### **Phase 1: Data Preparation (CRITICAL - Do This First!)**

#### ‚òê Step 1.1: Annotate Missing Entity Types
**Priority: CRITICAL**

Create at least 15-20 examples for each missing entity:

**letter_type** (‡∂Ö‡∂∫‡∂Ø‡∑î‡∂∏‡∑ä‡∂¥‡∂≠‡∑ä, ‡∂â‡∂Ω‡∑ä‡∂Ω‡∑ì‡∂∏‡∑ä, etc.):
- [ ] Find/create 20 examples with clear letter type mentions
- [ ] Annotate spans: "‡∂±‡∑í‡∑Ä‡∑è‡∂©‡∑î ‡∂â‡∂Ω‡∑ä‡∂Ω‡∑ì‡∂∏‡∑ä ‡∂Ω‡∑í‡∂¥‡∑í‡∂∫‡∂ö‡∑ä", "‡∂ª‡∑ê‡∂ö‡∑í‡∂∫‡∑è ‡∂Ö‡∂∫‡∂Ø‡∑î‡∂∏‡∑ä‡∂¥‡∂≠‡∑ä", "‡∂¥‡∑ê‡∂∏‡∑í‡∂´‡∑í‡∂Ω‡∑ä‡∂Ω‡∂ö‡∑ä"
- [ ] Add to `raw/train_samples.json`

**qualifications** (‡∂Ö‡∂∞‡∑ä‚Äç‡∂∫‡∑è‡∂¥‡∂± ‡∑É‡∑î‡∂Ø‡∑î‡∑É‡∑î‡∂ö‡∂∏‡∑ä, ‡∂¥‡∑Ö‡∂¥‡∑î‡∂ª‡∑î‡∂Ø‡∑ä‡∂Ø):
- [ ] Find/create 15 job application letters
- [ ] Annotate education, experience, skills sections
- [ ] Add to training data

**requested_action** (‡∂ö‡∂ª‡∑î‡∂´‡∑è‡∂ö‡∂ª‡∑Ä ‡∂Ö‡∂±‡∑î‡∂∏‡∂≠ ‡∂ö‡∂ª‡∂±‡∑ä‡∂±, ‡∑Ä‡∑í‡∂∏‡∂ª‡∑ä‡∑Å‡∂±‡∂∫‡∂ö‡∑ä ‡∑É‡∑í‡∂Ø‡∑î ‡∂ö‡∂ª‡∂±‡∑ä‡∂±):
- [ ] Find 15 letters with clear action requests
- [ ] Annotate: "‡∂Ö‡∂±‡∑î‡∂∏‡∑ê‡∂≠‡∑í‡∂∫ ‡∂Ω‡∂∂‡∑è ‡∂Ø‡∑ô‡∂±‡∑ä‡∂±", "‡∂¥‡∂ª‡∑ì‡∂ö‡∑ä‡∑Ç‡∂´‡∂∫‡∂ö‡∑ä ‡∂ö‡∂ª‡∂±‡∑ä‡∂±"
- [ ] Add to training data

**requested_items** (‡∂¥‡∑ú‡∂≠‡∑ä, ‡∂¥‡∂ª‡∑í‡∂ú‡∂´‡∂ö ‡∂ã‡∂¥‡∂ö‡∂ª‡∂´):
- [ ] Find 15 donation/request letters
- [ ] Annotate item lists and descriptions
- [ ] Add to training data

**timeline** (‡∂Ø‡∑í‡∂± ‡∂¥‡∂ª‡∑è‡∑É‡∂∫, ‡∂ö‡∑è‡∂Ω ‡∑É‡∑ì‡∂∏‡∑è‡∑Ä):
- [ ] Find 20 examples with dates, deadlines
- [ ] Annotate: "‡∂¢‡∂±‡∑Ä‡∑è‡∂ª‡∑í 20 ‡∑É‡∑í‡∂ß 25 ‡∂Ø‡∂ö‡∑ä‡∑Ä‡∑è", "‡∂∏‡∑è‡∂ª‡∑ä‡∂≠‡∑î 31 ‡∑Ä‡∂± ‡∑Ä‡∑í‡∂ß"
- [ ] Add to training data

#### ‚òê Step 1.2: Balance Existing Entity Types
**Priority: HIGH**

- [ ] Add 20-30 more samples for `details` (currently only 10)
- [ ] Add 10-15 more samples for `contact_details` (currently only 11)
- [ ] Ensure all entity types have at least 30 samples each

#### ‚òê Step 1.3: Expand Training Dataset
**Priority: MEDIUM-HIGH**

**Target: 200-300 total annotated samples**

Options:
1. **Manual Annotation** (Recommended for quality)
   - [ ] Use existing CSV data (12 letters)
   - [ ] Find real Sinhala letters from online sources
   - [ ] Create synthetic variations of existing letters
   - [ ] Annotate using a simple annotation tool

2. **Synthetic Data Generation**
   - [ ] Use LLM to generate synthetic Sinhala letters
   - [ ] Manually verify and annotate generated letters
   - [ ] Focus on underrepresented entity types

3. **Data Augmentation**
   - [ ] Synonym replacement for Sinhala words
   - [ ] Sentence reordering (maintain entity spans)
   - [ ] Back-translation (Sinhala ‚Üí English ‚Üí Sinhala)

#### ‚òê Step 1.4: Quality Check Annotations
**Priority: CRITICAL**

- [ ] Review all annotated spans for accuracy
- [ ] Check entity boundaries (complete words, no partial words)
- [ ] Verify consistent annotation conventions
- [ ] Fix any mislabeled entities
- [ ] Remove duplicates

#### ‚òê Step 1.5: Prepare Transformer Format
**Priority: HIGH**

After adding new samples:
```bash
python rag/models/prepare_ner_dataset.py
```

- [ ] Run dataset preparation script
- [ ] Verify transformer format files created
- [ ] Check token-label alignment
- [ ] Review BIO tagging format (B-entity, I-entity, O)

---

### **Phase 2: Environment Setup**

#### ‚òê Step 2.1: Install Required Packages
```bash
pip install transformers torch tqdm scikit-learn seqeval
```

- [ ] Install PyTorch with CUDA (if GPU available) or CPU version
- [ ] Install transformers library
- [ ] Install seqeval (for NER metrics)
- [ ] Install tqdm (progress bars)

#### ‚òê Step 2.2: Verify GPU Availability (Optional but Recommended)
```python
python -c "import torch; print(f'GPU Available: {torch.cuda.is_available()}')"
```

- [ ] Check if CUDA GPU is available
- [ ] If yes, training will be 10-100x faster
- [ ] If no, training on CPU will be slow but possible

---

### **Phase 3: Model Training**

#### ‚òê Step 3.1: Configure Training Parameters

**Edit config if needed** (defaults are reasonable):
- `epochs`: 3-5 (start with 3)
- `batch_size`: 8 (reduce to 4 if GPU memory issues)
- `learning_rate`: 2e-5 (standard for fine-tuning)
- `model`: "xlm-roberta-base" (good for Sinhala)

#### ‚òê Step 3.2: Run Initial Training
```bash
cd C:\MSC\code\enhanceLetterWritingSinhala
python rag/finetune_ner_model.py --epochs 3 --batch-size 8
```

**Expected Duration:**
- GPU: 10-30 minutes for 3 epochs with 100 samples
- CPU: 1-3 hours for 3 epochs with 100 samples

Watch for:
- [ ] Training loss decreasing each epoch
- [ ] Validation loss decreasing (not increasing = overfitting)
- [ ] Final F1 score > 0.70 is acceptable
- [ ] Final F1 score > 0.85 is good

#### ‚òê Step 3.3: Monitor Training Metrics

During training, check:
- [ ] Loss curves (should decrease steadily)
- [ ] Per-entity F1 scores
- [ ] Precision and recall balance
- [ ] No significant overfitting (train vs val gap)

#### ‚òê Step 3.4: Save Best Model

The script should save the best model to:
```
rag/models/training_data/best_model/
```

- [ ] Verify model files created
- [ ] Check model size (~550 MB for XLM-RoBERTa)
- [ ] Save training logs and metrics

---

### **Phase 4: Evaluation & Testing**

#### ‚òê Step 4.1: Test NER Model
```bash
python rag/test_ner_model.py
```

- [ ] Test with example prompts (English and Sinhala)
- [ ] Verify all entity types are extracted
- [ ] Check extraction quality (correct spans, no hallucinations)
- [ ] Test edge cases (short prompts, mixed language, etc.)

#### ‚òê Step 4.2: Calculate Metrics

Key metrics to check:
- [ ] **Overall F1 Score**: Should be > 0.70 (acceptable), > 0.85 (good)
- [ ] **Per-Entity F1 Scores**: Check each entity type individually
- [ ] **Precision**: Fewer false positives = higher precision
- [ ] **Recall**: Fewer missed entities = higher recall

#### ‚òê Step 4.3: Error Analysis

- [ ] Identify which entity types have low F1 scores
- [ ] Check common error patterns (confusion matrix)
- [ ] Review false positives (model extracts wrong text)
- [ ] Review false negatives (model misses correct entities)
- [ ] Document problematic cases for data augmentation

---

### **Phase 5: Integration & Deployment**

#### ‚òê Step 5.1: Update Config to Use NER

Edit `rag/config.py`:
```python
use_ner_extraction: bool = True  # Change from False to True
```

- [ ] Enable NER extraction in config
- [ ] Restart the FastAPI server
- [ ] Verify "NER model loaded successfully" in logs

#### ‚òê Step 5.2: Test End-to-End Pipeline

- [ ] Start server: `python -m uvicorn sinhala_letter_rag:app`
- [ ] Test with `test_api.html` in browser
- [ ] Try various Sinhala prompts
- [ ] Verify extracted_info fields are correct
- [ ] Check that LLM fallback doesn't trigger unnecessarily

#### ‚òê Step 5.3: Benchmark Performance

Compare NER vs LLM extraction:
- [ ] Accuracy: NER should be more accurate on trained entity types
- [ ] Speed: NER should be 10-100x faster than LLM
- [ ] Cost: NER is free, LLM uses Ollama (also free but slower)
- [ ] Completeness: LLM may handle edge cases better initially

---

### **Phase 6: Iteration & Improvement (Optional but Recommended)**

#### ‚òê Step 6.1: Collect More Data

- [ ] Monitor production errors (entity extraction failures)
- [ ] Collect real user prompts
- [ ] Annotate problematic examples
- [ ] Add to training set

#### ‚òê Step 6.2: Re-train with More Data

- [ ] Add new annotations (target: 500-1000 samples)
- [ ] Re-run training with more epochs (5-10)
- [ ] Evaluate improved model
- [ ] Deploy if metrics improve

#### ‚òê Step 6.3: Try Advanced Techniques

**If F1 score < 0.70 after initial training:**

1. **Data Augmentation**
   - [ ] Implement Sinhala-specific augmentation
   - [ ] Use back-translation
   - [ ] Generate synthetic examples with LLM

2. **Model Architecture**
   - [ ] Try larger models (xlm-roberta-large)
   - [ ] Try specialized models (multilingual BERT variants)
   - [ ] Experiment with character-level models

3. **Training Optimization**
   - [ ] Increase epochs (up to 10-15)
   - [ ] Try different learning rates (1e-5 to 5e-5)
   - [ ] Implement learning rate scheduling
   - [ ] Use gradient accumulation for larger effective batch size

4. **Hybrid Approach**
   - [ ] Use NER for common entities
   - [ ] Use LLM fallback for rare/unseen entities
   - [ ] Combine NER + rule-based + LLM (ensemble)

---

## üéØ **Minimum Viable Training Plan (Quick Start)**

If you want to get started quickly with limited resources:

### Quick Path (2-3 days):

1. **Day 1: Data Preparation (6-8 hours)**
   - Annotate 15-20 samples for each missing entity type (~75-100 new samples)
   - Use existing CSV data + find 20-30 real Sinhala letters online
   - Total target: ~150 annotated samples (up from 64)

2. **Day 2: Training & Testing (2-3 hours)**
   - Run `prepare_ner_dataset.py` to create transformer format
   - Train model: `python rag/finetune_ner_model.py --epochs 5`
   - Test model: `python rag/test_ner_model.py`
   - If F1 > 0.65, proceed. If not, add more data.

3. **Day 3: Integration & Evaluation (2-3 hours)**
   - Enable NER in config
   - Test end-to-end with various prompts
   - Compare NER vs LLM extraction quality
   - Document results for thesis

---

## üìù **Annotation Guidelines**

When annotating new training samples in `raw/train_samples.json`:

```json
{
  "text": "‡∂¢‡∂±‡∑Ä‡∑è‡∂ª‡∑í 20 ‡∑É‡∑í‡∂ß ‡∂¢‡∂±‡∑Ä‡∑è‡∂ª‡∑í 25 ‡∂Ø‡∂ö‡∑ä‡∑Ä‡∑è ‡∑Ä‡∑è‡∂ª‡∑ä‡∑Ç‡∑í‡∂ö ‡∂±‡∑í‡∑Ä‡∑è‡∂©‡∑î ‡∑É‡∂≥‡∑Ñ‡∑è ‡∂∏‡∂ú‡∑ö ‡∂ö‡∑Ö‡∂∏‡∂±‡∑è‡∂ö‡∂ª‡∑î‡∂ß ‡∑Ä‡∑í‡∂∞‡∑í‡∂∏‡∂≠‡∑ä ‡∂±‡∑í‡∑Ä‡∑è‡∂©‡∑î ‡∂â‡∂Ω‡∑ä‡∂Ω‡∑ì‡∂∏‡∑ä ‡∂Ω‡∑í‡∂¥‡∑í‡∂∫‡∂ö‡∑ä ‡∂Ω‡∑í‡∂∫‡∂±‡∑ä‡∂±",
  "entities": {
    "letter_type": {
      "text": "‡∂±‡∑í‡∑Ä‡∑è‡∂©‡∑î ‡∂â‡∂Ω‡∑ä‡∂Ω‡∑ì‡∂∏‡∑ä ‡∂Ω‡∑í‡∂¥‡∑í‡∂∫‡∂ö‡∑ä",
      "start": 74,
      "end": 96
    },
    "recipient": {
      "text": "‡∂∏‡∂ú‡∑ö ‡∂ö‡∑Ö‡∂∏‡∂±‡∑è‡∂ö‡∂ª‡∑î‡∂ß",
      "start": 54,
      "end": 68
    },
    "timeline": {
      "text": "‡∂¢‡∂±‡∑Ä‡∑è‡∂ª‡∑í 20 ‡∑É‡∑í‡∂ß ‡∂¢‡∂±‡∑Ä‡∑è‡∂ª‡∑í 25 ‡∂Ø‡∂ö‡∑ä‡∑Ä‡∑è",
      "start": 0,
      "end": 30
    },
    "purpose": {
      "text": "‡∑Ä‡∑è‡∂ª‡∑ä‡∑Ç‡∑í‡∂ö ‡∂±‡∑í‡∑Ä‡∑è‡∂©‡∑î ‡∑É‡∂≥‡∑Ñ‡∑è",
      "start": 31,
      "end": 48
    }
  }
}
```

**Rules:**
1. `text`: Complete letter or prompt text
2. `entities`: Dictionary of entity_type ‚Üí {text, start, end}
3. `start`/`end`: Character positions (0-indexed)
4. Extract complete words/phrases (no partial words)
5. Be consistent with entity boundaries

---

## üîß **Troubleshooting Common Issues**

### Issue 1: "CUDA out of memory"
**Solution:** Reduce batch size to 4 or 2, or use CPU

### Issue 2: "F1 score is very low (< 0.40)"
**Solution:** Need more training data (at least 200 samples)

### Issue 3: "Model extracts entire prompt as one entity"
**Solution:** Likely insufficient or incorrect annotations. Review training data quality.

### Issue 4: "Model doesn't extract rare entity types"
**Solution:** Add more examples for those entity types (minimum 20 per type)

### Issue 5: "Training takes too long (> 3 hours)"
**Solution:** Use GPU or reduce dataset size for initial testing

---

## üìö **Resources & References**

1. **Sinhala NLP Resources:**
   - XLM-RoBERTa: https://huggingface.co/xlm-roberta-base
   - Multilingual BERT: https://huggingface.co/bert-base-multilingual-cased

2. **Annotation Tools:**
   - Label Studio: https://labelstud.io/
   - Doccano: https://github.com/doccano/doccano
   - Simple JSON editor: Just edit `train_samples.json` manually

3. **Evaluation Metrics:**
   - seqeval library: https://github.com/chakki-works/seqeval
   - NER metrics explained: https://towardsdatascience.com/evaluation-metrics-for-ner-f69b62e1e24

---

## ‚ú® **Expected Results After Training**

**With 150-200 well-annotated samples:**
- Overall F1: 0.65-0.75 (acceptable for MVP)
- Extraction speed: < 100ms per prompt
- Accuracy: 70-80% on test set

**With 500-1000 well-annotated samples:**
- Overall F1: 0.80-0.90 (production-quality)
- Extraction speed: < 100ms per prompt
- Accuracy: 85-95% on test set

**Benefits over LLM-only approach:**
- 10-100x faster extraction
- More consistent results
- No API costs (fully local)
- Better for thesis research (ML component)

---

## üéì **Academic/Thesis Value**

**Why train NER for your MSc research:**

1. **Novel Contribution:** Custom NER for Sinhala formal letters (limited prior work)
2. **Comparative Study:** Compare NER vs LLM vs Hybrid approaches
3. **Quantitative Results:** F1 scores, precision, recall metrics
4. **Ablation Studies:** Impact of training data size, entity types, model architecture
5. **Efficiency Analysis:** Speed and resource usage comparison

**Thesis Sections this enables:**
- Methodology: NER model architecture and training
- Experiments: Baseline vs. enhanced pipeline comparison
- Results: Quantitative metrics (F1, latency, cost)
- Discussion: Trade-offs between approaches

---

## üöÄ **Next Actions**

**Immediate (This Week):**
1. [ ] Annotate 15-20 examples for each missing entity type
2. [ ] Add annotations to `raw/train_samples.json`
3. [ ] Run `prepare_ner_dataset.py`

**Short-term (Next Week):**
4. [ ] Train initial model with ~150 samples
5. [ ] Evaluate model performance
6. [ ] Test end-to-end integration

**Medium-term (Next 2 weeks):**
7. [ ] Collect more training data (target: 300-500 samples)
8. [ ] Re-train with expanded dataset
9. [ ] Run comparative experiments (NER vs LLM vs Hybrid)
10. [ ] Document results for thesis

---

## üí° **Alternative: LLM-Based Extraction (Current Approach)**

**If training NER is too time-consuming for your project timeline:**

‚úÖ **Pros:**
- Already working with Ollama
- No training data needed
- Handles unseen entity types
- Zero-shot learning

‚ùå **Cons:**
- Slower (2-10 seconds per extraction)
- Less consistent results
- Harder to debug
- Less impressive for thesis (no custom ML model)

**You can continue using LLM extraction for your MVP and add NER training as a "future work" section in your thesis.**

---

**Questions? Check the code:**
- Training script: `rag/finetune_ner_model.py`
- NER model: `rag/models/sinhala_ner.py`
- Dataset prep: `rag/models/prepare_ner_dataset.py`
- Test script: `rag/test_ner_model.py`
